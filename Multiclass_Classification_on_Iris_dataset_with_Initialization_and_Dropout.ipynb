{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, x_val, y, y_val = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,), (50, 4), (50,))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.reshape(-1, x.shape[1]).astype('float32')\n",
    "y_train = y\n",
    "\n",
    "x_val = x_val.reshape(-1, x_val.shape[1]).astype('float32')\n",
    "y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define validation data as a Pytorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = torch.from_numpy(x_val)\n",
    "y_val = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation f√ºr DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put Data through DataLoader, so we can use batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x=torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        self.y=torch.from_numpy(y).type(torch.LongTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=DataLoader(dataset=data_set,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.7000, 2.9000, 4.2000, 1.3000],\n",
       "        [7.6000, 3.0000, 6.6000, 2.1000],\n",
       "        [5.6000, 3.0000, 4.5000, 1.5000],\n",
       "        [5.1000, 3.5000, 1.4000, 0.2000],\n",
       "        [7.7000, 2.8000, 6.7000, 2.0000],\n",
       "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
       "        [5.2000, 3.4000, 1.4000, 0.2000],\n",
       "        [5.0000, 3.5000, 1.3000, 0.3000],\n",
       "        [5.1000, 3.8000, 1.9000, 0.4000],\n",
       "        [5.0000, 2.0000, 3.5000, 1.0000]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.x[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4]), torch.Size([100]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.x.shape, data_set.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_size,n_hidden1,n_hidden2,out_size,p=0):\n",
    "\n",
    "        super(Net,self).__init__()\n",
    "        self.drop=nn.Dropout(p=p)\n",
    "        self.linear1=nn.Linear(in_size,n_hidden1)\n",
    "        nn.init.kaiming_uniform_(self.linear1.weight,nonlinearity='relu')\n",
    "        self.linear2=nn.Linear(n_hidden1,n_hidden2)\n",
    "        nn.init.kaiming_uniform_(self.linear1.weight,nonlinearity='relu')\n",
    "        self.linear3=nn.Linear(n_hidden2,n_hidden2)\n",
    "        nn.init.kaiming_uniform_(self.linear3.weight,nonlinearity='relu')\n",
    "        self.linear4=nn.Linear(n_hidden2,out_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.drop(x)\n",
    "        x=F.relu(self.linear2(x))\n",
    "        x=self.drop(x)\n",
    "        x=F.relu(self.linear3(x))\n",
    "        x=self.drop(x)\n",
    "        x=self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (linear1): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (linear3): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (linear4): Linear(in_features=5, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Net(4,10,5,3)\n",
    "model_drop=Net(4,10,5,3,p=0.2)\n",
    "model_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (linear1): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (linear3): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (linear4): Linear(in_features=5, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_drop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS={}\n",
    "LOSS['training data no dropout']=[]\n",
    "LOSS['validation data no dropout']=[]\n",
    "LOSS['training data dropout']=[]\n",
    "LOSS['validation data dropout']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.2261919975280762\n",
      "epoch 0, loss 1.1832058429718018\n",
      "epoch 0, loss 1.1479732990264893\n",
      "epoch 0, loss 1.1195118427276611\n",
      "epoch 0, loss 1.0988399982452393\n",
      "epoch 0, loss 1.0859699249267578\n",
      "epoch 0, loss 1.0753216743469238\n",
      "epoch 0, loss 1.0665377378463745\n",
      "epoch 0, loss 1.0593410730361938\n",
      "epoch 0, loss 1.053403615951538\n",
      "epoch 1, loss 1.0483787059783936\n",
      "epoch 1, loss 1.0439419746398926\n",
      "epoch 1, loss 1.0398415327072144\n",
      "epoch 1, loss 1.0357444286346436\n",
      "epoch 1, loss 1.031473159790039\n",
      "epoch 1, loss 1.0264718532562256\n",
      "epoch 1, loss 1.020595908164978\n",
      "epoch 1, loss 1.013972282409668\n",
      "epoch 1, loss 1.006752371788025\n",
      "epoch 1, loss 0.9985687732696533\n",
      "epoch 2, loss 0.9888967275619507\n",
      "epoch 2, loss 0.9779769778251648\n",
      "epoch 2, loss 0.9661227464675903\n",
      "epoch 2, loss 0.9531617760658264\n",
      "epoch 2, loss 0.9389583468437195\n",
      "epoch 2, loss 0.9240256547927856\n",
      "epoch 2, loss 0.910552442073822\n",
      "epoch 2, loss 0.8976549506187439\n",
      "epoch 2, loss 0.885875403881073\n",
      "epoch 2, loss 0.8734304904937744\n",
      "epoch 3, loss 0.8601499199867249\n",
      "epoch 3, loss 0.8471277356147766\n",
      "epoch 3, loss 0.834900975227356\n",
      "epoch 3, loss 0.8223391175270081\n",
      "epoch 3, loss 0.8105682134628296\n",
      "epoch 3, loss 0.7995368242263794\n",
      "epoch 3, loss 0.7890735864639282\n",
      "epoch 3, loss 0.7789437174797058\n",
      "epoch 3, loss 0.7690907120704651\n",
      "epoch 3, loss 0.759411096572876\n",
      "epoch 4, loss 0.7498090863227844\n",
      "epoch 4, loss 0.74030601978302\n",
      "epoch 4, loss 0.7311858534812927\n",
      "epoch 4, loss 0.7224366068840027\n",
      "epoch 4, loss 0.7142775654792786\n",
      "epoch 4, loss 0.7069597840309143\n",
      "epoch 4, loss 0.7004395127296448\n",
      "epoch 4, loss 0.6944023966789246\n",
      "epoch 4, loss 0.6884732246398926\n",
      "epoch 4, loss 0.6824700832366943\n",
      "epoch 5, loss 0.6763313412666321\n",
      "epoch 5, loss 0.6700398921966553\n",
      "epoch 5, loss 0.6638635396957397\n",
      "epoch 5, loss 0.6579897403717041\n",
      "epoch 5, loss 0.652558445930481\n",
      "epoch 5, loss 0.6474823951721191\n",
      "epoch 5, loss 0.6424949169158936\n",
      "epoch 5, loss 0.637371838092804\n",
      "epoch 5, loss 0.6320363879203796\n",
      "epoch 5, loss 0.6265389919281006\n",
      "epoch 6, loss 0.6210318803787231\n",
      "epoch 6, loss 0.6156303882598877\n",
      "epoch 6, loss 0.6102628707885742\n",
      "epoch 6, loss 0.604780375957489\n",
      "epoch 6, loss 0.5990113615989685\n",
      "epoch 6, loss 0.5928799510002136\n",
      "epoch 6, loss 0.5862977504730225\n",
      "epoch 6, loss 0.579251229763031\n",
      "epoch 6, loss 0.5716457962989807\n",
      "epoch 6, loss 0.5633577108383179\n",
      "epoch 7, loss 0.5542782545089722\n",
      "epoch 7, loss 0.544497013092041\n",
      "epoch 7, loss 0.5349540710449219\n",
      "epoch 7, loss 0.5255962610244751\n",
      "epoch 7, loss 0.5160695910453796\n",
      "epoch 7, loss 0.506332278251648\n",
      "epoch 7, loss 0.496280699968338\n",
      "epoch 7, loss 0.4859603941440582\n",
      "epoch 7, loss 0.47549575567245483\n",
      "epoch 7, loss 0.46486756205558777\n",
      "epoch 8, loss 0.4539836049079895\n",
      "epoch 8, loss 0.4428432881832123\n",
      "epoch 8, loss 0.4315783977508545\n",
      "epoch 8, loss 0.4203546643257141\n",
      "epoch 8, loss 0.40926194190979004\n",
      "epoch 8, loss 0.3983268737792969\n",
      "epoch 8, loss 0.38754233717918396\n",
      "epoch 8, loss 0.37702271342277527\n",
      "epoch 8, loss 0.36684250831604004\n",
      "epoch 8, loss 0.3570637106895447\n",
      "epoch 9, loss 0.3476197421550751\n",
      "epoch 9, loss 0.33848780393600464\n",
      "epoch 9, loss 0.3296909034252167\n",
      "epoch 9, loss 0.3212336003780365\n",
      "epoch 9, loss 0.3130424916744232\n",
      "epoch 9, loss 0.30501848459243774\n",
      "epoch 9, loss 0.2971349060535431\n",
      "epoch 9, loss 0.28942954540252686\n",
      "epoch 9, loss 0.28187182545661926\n",
      "epoch 9, loss 0.2744266092777252\n",
      "epoch 10, loss 0.2670556902885437\n",
      "epoch 10, loss 0.2597807049751282\n",
      "epoch 10, loss 0.2525646388530731\n",
      "epoch 10, loss 0.24542468786239624\n",
      "epoch 10, loss 0.23834118247032166\n",
      "epoch 10, loss 0.23138366639614105\n",
      "epoch 10, loss 0.22455114126205444\n",
      "epoch 10, loss 0.217803955078125\n",
      "epoch 10, loss 0.2111218422651291\n",
      "epoch 10, loss 0.20454445481300354\n",
      "epoch 11, loss 0.19810664653778076\n",
      "epoch 11, loss 0.19178493320941925\n",
      "epoch 11, loss 0.18565750122070312\n",
      "epoch 11, loss 0.17994344234466553\n",
      "epoch 11, loss 0.17439918220043182\n",
      "epoch 11, loss 0.16899992525577545\n",
      "epoch 11, loss 0.16375964879989624\n",
      "epoch 11, loss 0.15868400037288666\n",
      "epoch 11, loss 0.15373699367046356\n",
      "epoch 11, loss 0.14892728626728058\n",
      "epoch 12, loss 0.14432771503925323\n",
      "epoch 12, loss 0.139917254447937\n",
      "epoch 12, loss 0.13557834923267365\n",
      "epoch 12, loss 0.1314091682434082\n",
      "epoch 12, loss 0.12744364142417908\n",
      "epoch 12, loss 0.12369421869516373\n",
      "epoch 12, loss 0.12013885378837585\n",
      "epoch 12, loss 0.11674956232309341\n",
      "epoch 12, loss 0.11349743604660034\n",
      "epoch 12, loss 0.11040851473808289\n",
      "epoch 13, loss 0.10745058208703995\n",
      "epoch 13, loss 0.10462553054094315\n",
      "epoch 13, loss 0.10190685093402863\n",
      "epoch 13, loss 0.09926094114780426\n",
      "epoch 13, loss 0.09676423668861389\n",
      "epoch 13, loss 0.099052295088768\n",
      "epoch 13, loss 0.09276288747787476\n",
      "epoch 13, loss 0.09039410948753357\n",
      "epoch 13, loss 0.08845104277133942\n",
      "epoch 13, loss 0.08674419671297073\n",
      "epoch 14, loss 0.08505973219871521\n",
      "epoch 14, loss 0.08346796035766602\n",
      "epoch 14, loss 0.08200071007013321\n",
      "epoch 14, loss 0.0805145725607872\n",
      "epoch 14, loss 0.07913138717412949\n",
      "epoch 14, loss 0.07782404124736786\n",
      "epoch 14, loss 0.07653787732124329\n",
      "epoch 14, loss 0.07536756247282028\n",
      "epoch 14, loss 0.07422608137130737\n",
      "epoch 14, loss 0.07315677404403687\n",
      "epoch 15, loss 0.07214010506868362\n",
      "epoch 15, loss 0.0711614340543747\n",
      "epoch 15, loss 0.07026322185993195\n",
      "epoch 15, loss 0.06937463581562042\n",
      "epoch 15, loss 0.06855115294456482\n",
      "epoch 15, loss 0.0677582249045372\n",
      "epoch 15, loss 0.06698963791131973\n",
      "epoch 15, loss 0.06627991050481796\n",
      "epoch 15, loss 0.06557664275169373\n",
      "epoch 15, loss 0.06492048501968384\n",
      "epoch 16, loss 0.06428395211696625\n",
      "epoch 16, loss 0.06366973370313644\n",
      "epoch 16, loss 0.06308793276548386\n",
      "epoch 16, loss 0.06251848489046097\n",
      "epoch 16, loss 0.061978522688150406\n",
      "epoch 16, loss 0.06145220622420311\n",
      "epoch 16, loss 0.06095466762781143\n",
      "epoch 16, loss 0.06047077476978302\n",
      "epoch 16, loss 0.06000654399394989\n",
      "epoch 16, loss 0.059559889137744904\n",
      "epoch 17, loss 0.05912412703037262\n",
      "epoch 17, loss 0.058707430958747864\n",
      "epoch 17, loss 0.05830332636833191\n",
      "epoch 17, loss 0.057915978133678436\n",
      "epoch 17, loss 0.05753680691123009\n",
      "epoch 17, loss 0.05717284604907036\n",
      "epoch 17, loss 0.05681629106402397\n",
      "epoch 17, loss 0.0564706027507782\n",
      "epoch 17, loss 0.05613778531551361\n",
      "epoch 17, loss 0.055813778191804886\n",
      "epoch 18, loss 0.055499739944934845\n",
      "epoch 18, loss 0.05519213154911995\n",
      "epoch 18, loss 0.05489616468548775\n",
      "epoch 18, loss 0.054606761783361435\n",
      "epoch 18, loss 0.05432669818401337\n",
      "epoch 18, loss 0.05405251681804657\n",
      "epoch 18, loss 0.05378641188144684\n",
      "epoch 18, loss 0.0535280667245388\n",
      "epoch 18, loss 0.05327662080526352\n",
      "epoch 18, loss 0.05303145572543144\n",
      "epoch 19, loss 0.05279258266091347\n",
      "epoch 19, loss 0.05256019905209541\n",
      "epoch 19, loss 0.05233333259820938\n",
      "epoch 19, loss 0.05211257562041283\n",
      "epoch 19, loss 0.051896873861551285\n",
      "epoch 19, loss 0.051686808466911316\n",
      "epoch 19, loss 0.05148143321275711\n",
      "epoch 19, loss 0.05128133296966553\n",
      "epoch 19, loss 0.05108543857932091\n",
      "epoch 19, loss 0.05089467391371727\n"
     ]
    }
   ],
   "source": [
    "n_epochs=20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in trainloader:\n",
    "        #make a prediction for both models \n",
    "        yhat = model(data_set.x)\n",
    "        yhat_drop = model_drop(data_set.x)\n",
    "        #calculate the lossf or both models \n",
    "        loss = criterion(yhat, data_set.y)\n",
    "        loss_drop = criterion(yhat_drop, data_set.y)\n",
    "\n",
    "        #store the loss for  both the training and validation  data for both models \n",
    "        LOSS['training data no dropout'].append(loss.item())\n",
    "        LOSS['training data dropout'].append(loss_drop.item())\n",
    "        model_drop.eval()\n",
    "        model_drop.train()\n",
    "\n",
    "        #clear gradient \n",
    "        optimizer_ofit.zero_grad()\n",
    "        optimizer_drop.zero_grad()\n",
    "        #Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "        loss.backward()\n",
    "        loss_drop.backward()\n",
    "        #the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer_ofit.step()\n",
    "        optimizer_drop.step()\n",
    "        \n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8671,  3.1897, -2.3804],\n",
       "        [-7.6598, -0.0838, 10.3663],\n",
       "        [-2.5126,  2.9921, -1.1193],\n",
       "        [ 4.9552, -3.4120, -5.1994],\n",
       "        [-7.8109, -0.1741, 10.7035],\n",
       "        [-1.8369,  3.1618, -2.4086],\n",
       "        [ 4.9045, -3.3656, -5.1474],\n",
       "        [ 4.8406, -3.3071, -5.0818],\n",
       "        [ 4.4102, -2.9133, -4.6403],\n",
       "        [-1.8369,  3.1618, -2.4086]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.1897, 10.3663,  2.9921,  4.9552, 10.7035,  3.1618,  4.9045,  4.8406,\n",
       "          4.4102,  3.1618,  3.4701,  4.2751,  4.2784,  3.9809,  3.2964,  4.7708,\n",
       "          3.1618,  7.7798,  4.4911,  3.1618,  7.6823,  5.7390,  2.1863,  6.3554,\n",
       "          3.1618,  2.8826,  7.2600,  3.1858,  4.8063,  3.1618,  6.1057,  4.7368,\n",
       "          4.8365,  3.1618,  2.4657,  4.5940,  8.4205,  4.7318,  4.0456,  2.3036,\n",
       "          3.1618,  8.2335,  3.2886,  4.2244,  9.4911,  3.2984,  4.1440,  4.6490,\n",
       "          4.2619,  4.6560,  4.3067,  4.7849,  4.3284,  2.2233,  8.9186,  4.5002,\n",
       "          6.6282, 10.7451,  4.6248,  3.1157,  3.1026,  2.7165,  3.1618,  7.3017,\n",
       "          5.0349,  5.4656,  3.2933,  4.8900,  3.1245,  3.2993,  3.3014,  3.5692,\n",
       "          3.2099,  3.2770,  5.1179,  3.1618,  7.3258,  6.4010,  4.6993,  3.1618,\n",
       "          9.9448,  2.8461,  4.5940,  9.5094,  4.4287,  3.3045,  7.4998,  5.9506,\n",
       "          3.1863,  4.5837,  3.1005,  3.1618,  6.2036,  3.8563,  4.6985,  3.1618,\n",
       "          4.4315,  5.8660,  3.1618,  7.9689]),\n",
       " tensor([1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2,\n",
       "         1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0,\n",
       "         2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0,\n",
       "         1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1,\n",
       "         2, 0, 1, 2]))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(yhat.data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model(x_val)\n",
    "z_dropout = model_drop(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,yhat=torch.max(z.data,1)\n",
    "yhat[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 1, 2, 0, 1, 1, 2, 1, 2, 0, 1, 0, 0, 1, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,yhat_dropout=torch.max(z_dropout.data,1)\n",
    "yhat_dropout[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0   0   1   2\n",
      "row_0            \n",
      "0      19   0   0\n",
      "1       0  15   0\n",
      "2       0   1  15\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "eval_matrix = (pd.crosstab(y_val, yhat))\n",
    "print(eval_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0   0   1  2\n",
      "row_0           \n",
      "0      18   1  0\n",
      "1       1  10  4\n",
      "2       0   8  8\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "eval_matrix_dropout = (pd.crosstab(y_val, yhat_dropout))\n",
    "print(eval_matrix_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(eval_matrix[0][0]+eval_matrix[1][1]+eval_matrix[2][2])/y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71999999999999997"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(eval_matrix_dropout[0][0]+eval_matrix_dropout[1][1]+eval_matrix_dropout[2][2])/y_val.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
